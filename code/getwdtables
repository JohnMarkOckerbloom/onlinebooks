#!/usr/bin/perl
use lib "nlib";
use LWP::UserAgent;
use JSON;
use OLBP;

my $idxdir = "/mnt/onlinebooks/nonpublic/bookdb/";
my $csvdir = "/home/LIBRARY/ockerblo/bibdata/wikipedia/";
my $packagedir = "/home/LIBRARY/ockerblo/bookdb/packages/wikipedia/";
my $hathidir = "/home/LIBRARY/ockerblo/bookdb/packages/hathitrust/";

my $bookdir = "/home/LIBRARY/ockerblo/bookdb/";

my $wikistage = "wikistage";
my $wikiprod  = "wikidata";

my $minwpfilesize = "90000";
my $maxwpfilesize = "900000";

my $mincsvfilesize = "9000000";
my $maxcsvfilesize = "20000000";

my $minarticlefilesize = "1600000";
my $maxarticlefilesize = "3000000";

my $minhathifilesize = "9000000";
my $maxhathifilesize = "20000000";

my $minbookfilesize = "18000000";
my $maxbookfilesize = "36000000";

my $csvstage   = "wikipstage.csv";
my $csvprod    = "wikiperiodicals.csv";

my $acsvstage   = "articlepstage.csv";
my $acsvprod    = "articleperiodicals.csv";

my $hcsvstage   = "wikihathistage.csv";
#my $hcsvprod    = "wikihathiserials.csv";
my $hcsvprod    = "wikihathioclcserials.csv";
#my $hcsvprod    = "wikihathiserial.csv";

my $bcsvstage   = "bookwikiidstage.csv";
my $bcsvprod    = "bookwikiids.csv";

my $agent = "DeepBackfile/0.1";

$COPYRIGHT_ID_QUERY = qq!SELECT ?olbpid ?wikidataid ?article 
       (group_concat(?issn) as ?issns)
WHERE
  {?wikidataid wdt:P5396 ?olbpid .
  OPTIONAL {?article schema:about ?wikidataid .
            ?article schema:inLanguage "en" .
            ?article schema:isPartOf <https://en.wikipedia.org/> }
  OPTIONAL {?wikidataid wdt:P236 ?issn}
}
GROUP BY ?olbpid ?wikidataid ?article
ORDER BY ?olbpid
!;

#$ISSN_QUERY = qq!SELECT ?wikidataid ?olbpid ?article ?start ?end
#       (group_concat(?issn) as ?issns)
#WHERE
#  {?wikidataid wdt:P236 ?issn .
#  OPTIONAL {?article schema:about ?wikidataid .
#            ?article schema:inLanguage "en" .
#            ?article schema:isPartOf <https://en.wikipedia.org/> }
#  OPTIONAL {?wikidataid wdt:P5396 ?olbpid}
#  OPTIONAL {?wikidataid wdt:P571 ?start}
#  OPTIONAL {?wikidataid wdt:P576 ?end}
#}
#GROUP BY ?wikidataid ?article ?olbpid ?start ?end
#ORDER BY ?wikidataid
#LIMIT 100000
#!;

$ISSN_QUERY = qq!SELECT ?wikidataid ?olbpid ?article ?start ?end
(group_concat(?issn) as ?issns)
WHERE
{
  {?wikidataid wdt:P236 ?issn} UNION {?wikidataid wdt:P5396 ?olbpid} .
  OPTIONAL {?article schema:about ?wikidataid .
            ?article schema:inLanguage "en" .
            ?article schema:isPartOf <https://en.wikipedia.org/> }
  OPTIONAL {?wikidataid wdt:P5396 ?olbpid}
  OPTIONAL {?wikidataid wdt:P571 ?start}
  OPTIONAL {?wikidataid wdt:P576 ?end}
}
GROUP BY ?wikidataid ?article ?olbpid ?start ?end
ORDER BY ?wikidataid
LIMIT 200000
!;

$ARTICLE_QUERY = qq!SELECT DISTINCT ?id ?article ?idLabel ?idDescription
   ?startdate ?enddate ?olbpid
   (GROUP_CONCAT(DISTINCT ?issn; SEPARATOR=" ") as ?issns)
WHERE
{
  ?article schema:about ?id .
  ?article schema:inLanguage "en" .
  ?article schema:isPartOf <https://en.wikipedia.org/> .
  {?id wdt:P31/wdt:P279* wd:Q1002697}
   UNION       {?id wdt:P236 ?issn} .
  OPTIONAL {?id wdt:P5396 ?olbpid}
  OPTIONAL {?id wdt:P571 ?startdate}
  OPTIONAL {?id wdt:P576 ?enddate}
  OPTIONAL {?id wdt:P582 ?enddate}
  OPTIONAL {?id wdt:P2669 ?enddate}

  FILTER (BOUND(?olbpid) || YEAR(?startdate) < 1964).
  SERVICE wikibase:label
     { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en" }
}
GROUP BY ?id ?article ?olbpid ?startdate ?enddate
         ?idLabel ?idDescription
LIMIT 100000
!;

$BOOK_QUERY = qq!SELECT DISTINCT ?id ?article
WHERE
{
  ?article schema:about ?id .
  ?article schema:inLanguage "en" .
  ?article schema:isPartOf <https://en.wikipedia.org/> .
  ?id wdt:P31/wdt:P279* wd:Q7725634

  SERVICE wikibase:label
     { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en" }
}
GROUP BY ?id ?article
LIMIT 300000
!;

#$HATHI_QUERY = qq!SELECT ?wikidataid ?olbpid ?issn
#       (group_concat(?hathitrustid) as ?hathitrustids)
#WHERE
#{
#  ?wikidataid wdt:P1844 ?hathitrustid;
#              wdt:P5396 ?olbpid .
#   OPTIONAL {?wikidataid wdt:P236 ?issn}
#}
#GROUP BY ?wikidataid ?olbpid ?issn
#ORDER BY ?wikidataid
#LIMIT 100000
#!;

$NEW_HATHI_QUERY = qq!SELECT ?wikidataid ?olbpid ?start ?end
(group_concat(?issn) as ?issns)
(group_concat(?hathiid) as ?hathiids)
WHERE
{
   wd:Q2217301 ^wdt:P279*/^wdt:P31 ?wikidataid .
  OPTIONAL {?wikidataid wdt:P236 ?issn}
  OPTIONAL {?wikidataid wdt:P1844 ?hathiid}
  OPTIONAL {?wikidataid wdt:P5396 ?olbpid}
  OPTIONAL {?wikidataid wdt:P571 ?start}
  OPTIONAL {?wikidataid wdt:P576 ?end}
  OPTIONAL {?wikidataid wdt:P582 ?end}
  OPTIONAL {?wikidataid wdt:P2669 ?end} 


  FILTER (BOUND(?olbpid) || BOUND(?hathiid) || BOUND(?issn))

}
GROUP BY ?wikidataid ?olbpid ?start ?end
ORDER BY ?wikidataid
LIMIT 200000
!;

$HATHI_OCLC_QUERY = qq!SELECT ?wikidataid ?olbpid
(group_concat(?issn) as ?issns)
(group_concat(?hathiid) as ?hathiids)
(group_concat(?oclc) as ?oclcs)
WHERE
{
   wd:Q2217301 ^wdt:P279*/^wdt:P31 ?wikidataid .
  OPTIONAL {?wikidataid wdt:P236 ?issn}
  OPTIONAL {?wikidataid wdt:P1844 ?hathiid}
  OPTIONAL {?wikidataid wdt:P5396 ?olbpid}
  OPTIONAL {?wikidataid wdt:P243 ?oclc}

  FILTER (BOUND(?olbpid) || BOUND(?hathiid) || BOUND(?issn) || BOUND(?oclc))

}
GROUP BY ?wikidataid ?olbpid
ORDER BY ?wikidataid
LIMIT 200000
!;
 
sub packhashtofile {
  my ($name, $hashref) = @_;
  my $fname = OLBP::hashfilename($name, $idxdir);
  my $hash = new OLBP::Hash(name=>$name, filename=>$fname);
  return $hash->pack_to_file(hash=>$hashref);
}


sub query_wikidata {
  my ($agent, $query, $format) = @_;
  my $endpointurl = "https://query.wikidata.org/sparql";
  my $queryURL = "${endpointurl}?query=${query}";
  my $ua = new LWP::UserAgent;
  $ua->agent($agent);
  my $req;
  if ($format eq "csv") {
    # CSV requests have to be specified in the header
    my $header = ["Accept"=>"text/csv"];
    $req = new HTTP::Request(GET, $queryURL, $header);
  } else {
    $queryURL .= "&format=${format}";
    $req = new HTTP::Request(GET=>$queryURL);
  }
  # Seem to be having problems with cert verification, so turning off for now
  # $ua->ssl_opts( "verify_hostname" => 0 );
  my $res = $ua->request($req);
  if ($res->is_success) {
     return $res->content;
  }
  print STDERR $res->status_line, "\n";
  exit 0;
}

sub make_idhash {
  my ($json) = @_;
  my $hash = {};
  return undef if (!$json);
  my $results = $json->{results};
  return undef if (!$results);
  my $bindings = $results->{bindings};
  return undef if (!$bindings);
  foreach my $binding (@{$bindings}) {
    my $olbpid = "";
    my $wdid = "";
    my $wparticle = "";
    if ($binding->{olbpid}) {
      $olbpid = $binding->{olbpid}->{value};
    }
    if ($binding->{wikidataid}) {
      $wdid = $binding->{wikidataid}->{value};
    }
    if ($binding->{article}) {
      $wparticle = $binding->{article}->{value};
    }
    if ($olbpid && $wdid) {
      $wdid =~ s/(.*)\///;          # remove path components
      $hash->{$olbpid} = "$wdid";
      if ($wparticle) {
        $hash->{$olbpid} .= " $wparticle";
      }
    }
  }
  return $hash;
}

sub shift_file {
  my ($oldpath, $newpath, $minsize, $maxsize) = @_;
  my $size = -s $oldpath;
  if (!$size || ($size < $minsize) || ($size > $maxsize)) {
    return 0;
  }
  rename($oldpath, $newpath);
  return 1;
}

sub writetofile {
  my ($name, $string) = @_;
  open my $fh, "> $name" or die "Can't open $name for writing";
  binmode $fh, ":utf8";
  print $fh $string;
  close $fh;
}

sub do_hathi_query {
  print "Doing a Hathitrust query\n";
  #  my $hathidata = query_wikidata($agent, $NEW_HATHI_QUERY, "csv");
  my $hathidata = query_wikidata($agent, $HATHI_OCLC_QUERY, "csv");
  my $csvpath = "$hathidir$hcsvstage";
  writetofile($csvpath, $hathidata);
  my $hpath = "$hathidir$hcsvprod";
  if (!shift_file("$csvpath", "$hpath", $minhathifilesize, $maxhathifilesize)) {
    print STDERR "Didn't like the look of $csvpath\n";
  }
  exit 0;
}

sub do_book_query {
  print "Doing a book query\n";
  my $bookdata = query_wikidata($agent, $BOOK_QUERY, "csv");
  my $csvpath = "$bookdir$bcsvstage";
  writetofile($csvpath, $bookdata);
  my $ppath = "$bookdir$bcsvprod";
  if (!shift_file("$csvpath", "$ppath", $minbookfilesize, $maxbookfilesize)) {
     print STDERR "Didn't like the look of $csvpath\n";
  }
  exit 0;
}




if ($ARGV[0] eq "hathi") {
  do_hathi_query;
}
if ($ARGV[0] eq "book") {
  do_book_query;
}

my $response = query_wikidata($agent, $COPYRIGHT_ID_QUERY, "json");
if ($response =~ /^C/) {
  die ($response);
}
my $data = JSON::decode_json($response);
my $idhash = make_idhash($data);
packhashtofile($wikistage, $idhash);
if (!shift_file("$idxdir$wikistage.hsh", "$idxdir$wikiprod.hsh",
    $minwpfilesize, $maxwpfilesize)) {
  print STDERR "Didn't like the look of $wikistage.hsh\n";
  exit 0;
} 
my $newdata = query_wikidata($agent, $ISSN_QUERY, "csv");
my $csvpath = "$csvdir$csvstage";
writetofile($csvpath, $newdata);
if (!shift_file("$csvdir$csvstage", "$csvdir$csvprod",
    $mincsvfilesize, $maxcsvfilesize)) {
  print STDERR "Didn't like the look of $csvstage.hsh\n";
  exit 0;
} 

my $moredata = query_wikidata($agent, $ARTICLE_QUERY, "csv");
$csvpath = "$packagedir$acsvstage";
writetofile($csvpath, $moredata);
my $newpath = "$packagedir$acsvprod";
if (!shift_file("$csvpath", "$newpath",
    $minarticlefilesize, $maxarticlefilesize)) {
  print STDERR "Didn't like the look of $csvpath\n";
  exit 0;
}

# my $hathidata = query_wikidata($agent, $HATHI_QUERY, "csv");
#$csvpath = "$hathidir$hcsvstage";
#writetofile($csvpath, $hathidata);
#my $hpath = "$hathidir$hcsvprod";
#if (!shift_file("$csvpath", "$hpath",
#    10000, 1000000)) {
#  print STDERR "Didn't like the look of $csvpath\n";
#  exit 0;
#}


