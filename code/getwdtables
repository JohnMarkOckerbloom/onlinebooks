#!/usr/bin/perl
use lib "nlib";
use LWP::UserAgent;
use JSON;
use OLBP;

my $idxdir = "/mnt/onlinebooks/nonpublic/bookdb/";
my $csvdir = "/home/LIBRARY/ockerblo/bibdata/wikipedia/";
my $packagedir = "/home/LIBRARY/ockerblo/bookdb/packages/wikipedia/";
my $hathidir = "/home/LIBRARY/ockerblo/bookdb/packages/hathitrust/";

my $wikistage = "wikistage";
my $wikiprod  = "wikidata";

my $minwpfilesize = "50000";
my $maxwpfilesize = "500000";

my $mincsvfilesize = "9000000";
my $maxcsvfilesize = "20000000";

my $minarticlefilesize = "1600000";
my $maxarticlefilesize = "3000000";

my $csvstage   = "wikipstage.csv";
my $csvprod    = "wikiperiodicals.csv";

my $acsvstage   = "articlepstage.csv";
my $acsvprod    = "articleperiodicals.csv";

my $hcsvstage   = "wikihathistage.csv";
my $hcsvprod    = "wikihathiserial.csv";

$COPYRIGHT_ID_QUERY = qq!SELECT ?olbpid ?wikidataid ?article 
       (group_concat(?issn) as ?issns)
WHERE
  {?wikidataid wdt:P5396 ?olbpid .
  OPTIONAL {?article schema:about ?wikidataid .
            ?article schema:inLanguage "en" .
            ?article schema:isPartOf <https://en.wikipedia.org/> }
  OPTIONAL {?wikidataid wdt:P236 ?issn}
}
GROUP BY ?olbpid ?wikidataid ?article
ORDER BY ?olbpid
!;

#$ISSN_QUERY = qq!SELECT ?wikidataid ?olbpid ?article ?start ?end
#       (group_concat(?issn) as ?issns)
#WHERE
#  {?wikidataid wdt:P236 ?issn .
#  OPTIONAL {?article schema:about ?wikidataid .
#            ?article schema:inLanguage "en" .
#            ?article schema:isPartOf <https://en.wikipedia.org/> }
#  OPTIONAL {?wikidataid wdt:P5396 ?olbpid}
#  OPTIONAL {?wikidataid wdt:P571 ?start}
#  OPTIONAL {?wikidataid wdt:P576 ?end}
#}
#GROUP BY ?wikidataid ?article ?olbpid ?start ?end
#ORDER BY ?wikidataid
#LIMIT 100000
#!;

$ISSN_QUERY = qq!SELECT ?wikidataid ?olbpid ?article ?start ?end
(group_concat(?issn) as ?issns)
WHERE
{
  {?wikidataid wdt:P236 ?issn} UNION {?wikidataid wdt:P5396 ?olbpid} .
  OPTIONAL {?article schema:about ?wikidataid .
            ?article schema:inLanguage "en" .
            ?article schema:isPartOf <https://en.wikipedia.org/> }
  OPTIONAL {?wikidataid wdt:P5396 ?olbpid}
  OPTIONAL {?wikidataid wdt:P571 ?start}
  OPTIONAL {?wikidataid wdt:P576 ?end}
}
GROUP BY ?wikidataid ?article ?olbpid ?start ?end
ORDER BY ?wikidataid
LIMIT 200000
!;

$ARTICLE_QUERY = qq!SELECT DISTINCT ?id ?article ?idLabel ?idDescription
   ?startdate ?enddate ?olbpid
   (GROUP_CONCAT(DISTINCT ?issn; SEPARATOR=" ") as ?issns)
WHERE
{
  ?article schema:about ?id .
  ?article schema:inLanguage "en" .
  ?article schema:isPartOf <https://en.wikipedia.org/> .
  {?id wdt:P31/wdt:P279* wd:Q1002697}
   UNION       {?id wdt:P236 ?issn} .
  OPTIONAL {?id wdt:P5396 ?olbpid}
  OPTIONAL {?id wdt:P571 ?startdate}
  OPTIONAL {?id wdt:P576 ?enddate}
  OPTIONAL {?id wdt:P582 ?enddate}
  OPTIONAL {?id wdt:P2669 ?enddate}

  FILTER (BOUND(?olbpid) || YEAR(?startdate) < 1964).
  SERVICE wikibase:label
     { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en" }
}
GROUP BY ?id ?article ?olbpid ?startdate ?enddate
         ?idLabel ?idDescription
LIMIT 100000
!;

$HATHI_QUERY = qq!SELECT ?wikidataid ?olbpid ?issn
       (group_concat(?hathitrustid) as ?hathitrustids)
WHERE
{
  ?wikidataid wdt:P1844 ?hathitrustid;
              wdt:P5396 ?olbpid .
   OPTIONAL {?wikidataid wdt:P236 ?issn}
}
GROUP BY ?wikidataid ?olbpid ?issn
ORDER BY ?wikidataid
LIMIT 100000
!;

sub packhashtofile {
  my ($name, $hashref) = @_;
  my $fname = OLBP::hashfilename($name, $idxdir);
  my $hash = new OLBP::Hash(name=>$name, filename=>$fname);
  return $hash->pack_to_file(hash=>$hashref);
}


sub query_wikidata {
  my ($agent, $query, $format) = @_;
  my $endpointurl = "https://query.wikidata.org/sparql";
  my $queryURL = "${endpointurl}?query=${query}";
  my $ua = new LWP::UserAgent;
  $ua->agent($agent);
  my $req;
  if ($format eq "csv") {
    # CSV requests have to be specified in the header
    my $header = ["Accept"=>"text/csv"];
    $req = new HTTP::Request(GET, $queryURL, $header);
  } else {
    $queryURL .= "&format=${format}";
    $req = new HTTP::Request(GET=>$queryURL);
  }
  my $res = $ua->request($req);
  my $str = $res->content;
  return $str;
}

sub make_idhash {
  my ($json) = @_;
  my $hash = {};
  return undef if (!$json);
  my $results = $json->{results};
  return undef if (!$results);
  my $bindings = $results->{bindings};
  return undef if (!$bindings);
  foreach my $binding (@{$bindings}) {
    my $olbpid = "";
    my $wdid = "";
    my $wparticle = "";
    if ($binding->{olbpid}) {
      $olbpid = $binding->{olbpid}->{value};
    }
    if ($binding->{wikidataid}) {
      $wdid = $binding->{wikidataid}->{value};
    }
    if ($binding->{article}) {
      $wparticle = $binding->{article}->{value};
    }
    if ($olbpid && $wdid) {
      $wdid =~ s/(.*)\///;          # remove path components
      $hash->{$olbpid} = "$wdid";
      if ($wparticle) {
        $hash->{$olbpid} .= " $wparticle";
      }
    }
  }
  return $hash;
}

sub shift_file {
  my ($oldpath, $newpath, $minsize, $maxsize) = @_;
  my $size = -s $oldpath;
  if (!$size || ($size < $minsize) || ($size > $maxsize)) {
    return 0;
  }
  rename($oldpath, $newpath);
  return 1;
}

sub writetofile {
  my ($name, $string) = @_;
  open my $fh, "> $name" or die "Can't open $name for writing";
  binmode $fh, ":utf8";
  print $fh $string;
  close $fh;
}


my $agent = "DeepBackfile/0.1";
my $data = JSON::decode_json(
               query_wikidata($agent, $COPYRIGHT_ID_QUERY, "json"));
my $idhash = make_idhash($data);
packhashtofile($wikistage, $idhash);
if (!shift_file("$idxdir$wikistage.hsh", "$idxdir$wikiprod.hsh",
    $minwpfilesize, $maxwpfilesize)) {
  print STDERR "Didn't like the look of $wikistage.hsh\n";
  exit 0;
} 
my $newdata = query_wikidata($agent, $ISSN_QUERY, "csv");
my $csvpath = "$csvdir$csvstage";
writetofile($csvpath, $newdata);
if (!shift_file("$csvdir$csvstage", "$csvdir$csvprod",
    $mincsvfilesize, $maxcsvfilesize)) {
  print STDERR "Didn't like the look of $csvstage.hsh\n";
  exit 0;
} 

my $moredata = query_wikidata($agent, $ARTICLE_QUERY, "csv");
$csvpath = "$packagedir$acsvstage";
writetofile($csvpath, $moredata);
my $newpath = "$packagedir$acsvprod";
if (!shift_file("$csvpath", "$newpath",
    $minarticlefilesize, $maxarticlefilesize)) {
  print STDERR "Didn't like the look of $csvpath\n";
  exit 0;
}

my $hathidata = query_wikidata($agent, $HATHI_QUERY, "csv");
$csvpath = "$hathidir$hcsvstage";
writetofile($csvpath, $hathidata);
my $hpath = "$hathidir$hcsvprod";
if (!shift_file("$csvpath", "$hpath",
    10000, 1000000)) {
  print STDERR "Didn't like the look of $csvpath\n";
  exit 0;
}


