#!/usr/bin/perl

use lib "nlib";
use POSIX qw(strftime);
use OLBP;
use OLBP::CopyrightInfo;
use OLBP::Entities;
use OLBP::PendingSerials;
use Text::CSV;

my $homedir = "/home/LIBRARY/ockerblo/";
my $cinfodir = "$homedir/digital/nonpublic/bookdb/cinfo/";
my $setdir = "$homedir/digital/nonpublic/bookdb/backfiles/";
my $datadir  = "$homedir/bookdb/";
my $wikidir  = "$homedir/bibdata/wikipedia/";
my $pooldir = "$homedir/digital/nonpublic/submissions/";
my $pkgdir   = $datadir . "packages/";

my $bookfile = $datadir . "wbookfile";
my $badissnfile = $datadir . "badissns";
my $wikidatafile = $wikidir . "wikiperiodicals.csv";
#my $hathisummaryfile = "$homedir/bibdata/hathifile/issnsfound";
my $hathisummaryfile = "$homedir/bibdata/hathifile/serialsfound2";
my $hathiussummaryfile = "$homedir/bibdata/hathifile/serialsfoundusoclc";
# my $hathiussummaryfile = "$homedir/bibdata/hathifile/serialsfoundusoclc-old";

my $olbpserial = {};
my $issnserial = {};
my $badissnmap = {};
my $badissnid = {};

my $cambridge_journals = $pkgdir . "cambridge/CUP_Cambridge_Journals _All_journals_2024-02-28.txt";
#my $cambridge_journals = $pkgdir . "cambridge/CUP_Cambridge_Journals _All_journals_2023-11-08.txt";

my $hathi_serials = $pkgdir . "hathitrust/wikihathioclcserials.csv";
my $hathipilot_serials = $pkgdir . "hathitrust/localplusus.tsv";
my $hathipilot_wdquery = $pkgdir . "hathitrust/wikihathiserial.csv";

#my $jstor_merged = $pkgdir . "jstor/JSTOR_ArchiveTitles_2023-11-12.txt";
my $jstor_merged = $pkgdir . "jstor/JSTOR_ArchiveTitles_2024-02-20.txt";
my $jstor_split = $pkgdir . "jstor/JSTOR_Global_AllArchiveTitles_2024-02-20.txt";
# my $jstor_split = $pkgdir . "jstor/JSTOR_Global_AllArchiveTitles_2023-11-12.txt";


my $muse_journals = $pkgdir . "muse/muse_journal_metadata_2019.tsv";

my $ovid_complete = $pkgdir . "wolters/Ovid_Global_AllJournals_2024-02-04.txt";
# my $ovid_complete = $pkgdir . "wolters/Ovid_Global_AllJournals_2023-05-22.txt";

#my $oxford_archive = $pkgdir . "oxford/OxfordUniversityPress_Global_2023JournalsArchiveAllTitles_2023-10-31.txt";
my $oxford_archive = $pkgdir . "oxford/OxfordUniversityPress_Global_2024JournalsArchiveAllTitles_2024-02-06.txt";

#my $penn_serials = $pkgdir . "penn/Serials before 1964.csv";
my $penn_serials = $pkgdir . "penn/Serials (All)  - Problem Columns Removed 052220.csv";
#my $penn2_serials = $pkgdir . "penn/Serials (All)  - Problem Columns Removed 052220 - Run 041221.csv";
my $penn2_serials = $pkgdir . "penn/Catch22.csv";

#my $proquest_archive = $pkgdir . "proquest/PeriodicalsArchiveOnline-2023.tsv";
my $proquest_archive = $pkgdir . "proquest/PeriodicalsArchiveOnline.tsv";

my $sage_journals = $pkgdir . "sage/deep_backfile_2024_list_0.txt";
# my $sage_journals = $pkgdir . "sage/7000031775_back23_complete_deep_backfile_purchase_2023_list.txt";

my $scidirect_journals = $pkgdir . "elsevier/ScienceDirectStandard_Global_ScienceDirectAvailableJournals_2024-02-28.txt";
#my $scidirect_journals = $pkgdir . "elsevier/ScienceDirectStandard_Global_ScienceDirectAvailableJournals_2023-11-09.txt";
my $sim_journals = $pkgdir . "sim/SIM-MASTER-TITLE-2017.txt";
#my $springer_journals = $pkgdir . "springer/Springer_Global_Complete_Journals_2022-12-12.txt";
my $springer_journals = $pkgdir . "springer/SpringerNature_Global_CompleteSpringerJournals_2024-02-13.txt";
my $taylor_global = $pkgdir . "taylor/TaylorandFrancisOnline_Global_DatabaseOAReadandPublish_2019-09-18.txt";
my $taylor_classic = $pkgdir . "taylor/ClassicAggregation.txt";
#my $taylor_classic = $pkgdir . "taylor/ClassicAggregation2023.txt";
my $wikipedia = $pkgdir . "wikipedia/articleperiodicals.csv";
my $wiley_all = $pkgdir . "wiley/wiley_alljournals_2024-02-01.TXT";
# my $wiley_all = $pkgdir . "wiley/wiley_alljournals_2023-01-02.TXT";

my $sagetitlefield = "Title";  # just a placeholder; varies by file

my $franklin_issn_search = "https://franklin.library.upenn.edu/catalog?utf8=%E2%9C%93&search_field=isxn_search&q=";

my $csv = Text::CSV->new ({ binary => 1, auto_diag => 1 });

my $tables = {
  "cambridge-journals" =>
     {file => $cambridge_journals, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&pre64andolbp},
  "hathitrust" =>
     {file => $hathi_serials, reader=>\&readcsvfile,
      parser=>\&htfilesparser, filter=>\&hathisortedfilter},
  "hathitrust-us" =>
     {file => $hathi_serials, reader=>\&readcsvfile,
      parser=>\&htfilesparser, filter=>\&hathisortedfilter},
  "hathitrust-pilot" =>
     {file => $hathipilot_serials, reader=>\&readtsvfile,
      parser=>\&htpilotparser, filter=>\&sortedfilterwitholbp},
  "hathitrust-pilot-test" =>
     {file => $hathipilot_serials, reader=>\&readtsvfile,
      parser=>\&htpilotparser, filter=>\&sort_titles},
  "jstor-consolidated-candidates" =>
     {file => $jstor_merged, reader=>\&readtsvfile,
      parser=>\&jstorparser, filter=>\&pre64andolbp},
  "jstor-separate-candidates" =>
     {file => $jstor_split, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&pre64andolbp},
  "muse-holdings" =>
     {file => $muse_journals, reader=>\&readmusefile,
      parser=>\&museshortparser, filter=>\&pre64andolbp},
  "muse-journals" =>
     {file => $muse_journals, reader=>\&readmusefile,
      parser=>\&muselongparser, filter=>\&pre64andolbp},
  "ovid-complete" =>
     {file => $ovid_complete, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&oxfordfilter},
  "oxford-archive" =>
     {file => $oxford_archive, reader=>\&readoxfordfile,
      parser=>\&kbartparser, filter=>\&oxfordfilter},
  "penn-serials" =>
     {file => $penn_serials, reader=>\&readcsvfile,
      parser=>\&pennparser, filter=>\&pennfilter},
  "penn-serials-test" =>
     {file => $penn2_serials, reader=>\&readcsvfile,
      parser=>\&pennparser, filter=>\&pennfilter},
  "proquest-archive" =>
     {file => $proquest_archive, reader=>\&readpqfile,
      parser=>\&pqparser, filter=>\&pre64andolbp},
  "sagepremier-available" =>
     {file => $sage_journals, reader=>\&readsagefile,
      parser=>\&sageparser, filter=>\&sortedfilter64},
  "scidirect-available" =>
     {file => $scidirect_journals, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&pre64andolbp},
  # below is for testing
  "scidirect-all" =>
     {file => $scidirect_journals, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&everything},
  "serials-microfilm" =>
     {file => $sim_journals, reader=>\&readsimfile,
      parser=>\&simparser, filter=>\&pre64andolbp},
  "springer-journals" =>
     {file => $springer_journals, reader=>\&readspringerfile,
      parser=>\&kbartparser, filter=>\&sortedfilter64},
  "taylor-global" =>
     {file => $taylor_global, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&pre64andolbp},
  "taylor-classic" =>
     {file => $taylor_classic, reader=>\&readtsvfile,
      parser=>\&kbartparser, filter=>\&sortedfilter64},
  # Wikipedia serials are pre-filtered
  "wikipedia-serials" =>
     {file => $wikipedia, reader=>\&readwikipediafile,
      parser=>\&wikipediaparser, filter=>\&wikipedia_filter},
  "wiley-all" =>
     {file => $wiley_all, reader=>\&readwileyfile,
      parser=>\&kbartparser, filter=>\&sortedfilter64}
};

my $renewals_start = $OLBP::currentyear - 95;
my $renewals_end   = 1963;

my $gapcode = "-||-";

# structure of records:
#  fields:
#   olbp : *serial* ID (not catalog record ID) in Online Books database
#   title : title of the serial
#   issns : hash ref; ISSNs associated with the serial are its hash keys
#   coverage : array ref; values are the year ranges after all SREFs
#   plus : true if there are other serials related to it with SREL
# structure of indexes:
#  $olbpserial: Indexed by OLBP serial ID; goes to record with the olbp value
#  $issnserial  Indexed by normalized ISSN; goes record with ISSN
  

sub normalize_issn {
  my ($str) = @_;
  return "" if (!$str);
  if ($str =~/(\d\d\d\d).*(\d\d\d[\dxX])/) {
    return ($1 . "-" . uc($2));
  }
  return "";
}

sub jstorparser {
  my ($set, @results) = @_;
  my @recs = ();
  foreach my $result (@results) {
    my $coverage = $result->{"Coverage Range (Years)"};
    my ($start, $end) = split /-/, $coverage;
    #if ($set eq "jstor-consolidated-candidates") {
    #  next if ($start > $renewals_end || $end < $renewals_start);
    #}
    my $title = $result->{Title};
    my @issns = ();
    if ($result->{ISSN}) {
      push @issns, normalize_issn($result->{ISSN});
    }
    if ($result->{eISSN}) {
      push @issns, normalize_issn($result->{eISSN});
    }
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    $rec->{title} = $title;
    $rec->{start} = $start;
    $rec->{end} = $end;
    $rec->{pkgid} = $result->{URL};
    push @recs, $rec;
  }
  return @recs;
}

sub stripspace {
  my $str = shift;
  $str =~ s/^\s+//;
  $str =~ s/\s+$//;
  return $str;
}

sub kbartparser {
  my ($set, @results) = @_;
  my @recs = ();
  foreach my $result (@results) {
    next if ($result->{publication_type} eq "Monograph");
    my $start = $result->{date_first_issue_online};
    my $end = stripspace($result->{date_last_issue_online});
    my $title = $result->{publication_title};
    my @issns = ();
    my $issn = stripspace($result->{print_identifier});
    my $eissn = stripspace($result->{online_identifier});
    if ($issn) {
      if (!($issn =~ /^\d\d\d\d-\d\d\d[\dXx]$/)) {
        print STDERR "Unexpected ISSN for $title: $issn\n";
      } else {
        push @issns, normalize_issn($issn);
      }
    }
    if ($eissn) {
      if (!($eissn =~ /^\d\d\d\d-\d\d\d[\dXx]$/)) {
        print STDERR "Unexpected eISSN for $title: $eissn\n";
      } else {
        push @issns, normalize_issn($eissn);
      }
    }
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    $rec->{title} = $title;
    # print "starting with $start for $title\n";
    # if we've seen a journal before (in the publication file,
    #  not in the Wikidata file), only update start if it's earlier
    # 
    if (!$rec->{seenpubstart} ||
        !$rec->{start} || $rec->{start} > int($start)) { 
      $rec->{start} = int($start);
      $rec->{seenpubstart} = 1;
    }
    $rec->{end} = ($end ? int($end) : "");
    $rec->{pkgid} = $result->{title_url};
    push @recs, $rec;
  }
  return @recs;
}

my $SAGESTUB = "https://us.sagepub.com/en-us/nam/product/";

sub sageparser {
  my ($set, @results) = @_;
  my @recs = ();
  # print STDERR "There are " . scalar(@results) . " results\n";
  foreach my $result (@results) {
    # print STDERR "Checking out $result\n";
    my $start = "";
    my $backstart = $result->{"Backfile First Issue Online"};
    if ($backstart =~ /(\d\d\d\d)\)/) {
      $start = $1;
    }
    # my $backend = $result->{"Backfile Last Issue Online"};
    # if ($backend =~ /(\d\d\d\d)\)/) {
    #   $end = $1;
    # }
    my $end = "1998";
    my $title = $result->{$sagetitlefield};
    $title =~ s/(.*), The$/The $1/;          # deinvert "The" titles
    my @issns = ();
    if ($result->{ISSN}) {
      push @issns, normalize_issn($result->{ISSN});
    }
    if ($result->{EISSN}) {
      push @issns, normalize_issn($result->{EISSN});
    }
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    my $url = $result->{URL};
    # my $url = $SAGESTUB . $issns[0];
    # print STDERR "$title is at $url and goes from $start to $end\n";
    $rec->{title} = $title;
    $rec->{start} = $start;
    $rec->{end} = $end;
    $rec->{pkgid} = $url;
    push @recs, $rec;
  }
  return @recs;
}

sub museparser {
  my ($set, $usewikidatastart, @results) = @_;
  my @recs = ();
  foreach my $result (@results) {
    my $start = "";
    my $end = "";
    my $backstart = $result->{"First Issue in MUSE"};
    if ($backstart =~ /(\d\d\d\d)\)/) {
      $start = $1;
    }
    my $backend = $result->{"Final Issue in MUSE"};
    if ($backend =~ /(\d\d\d\d)\)/) {
      $end = $1;
    }
    my $title = $result->{"Title"};
    my @issns = ();
    if ($result->{"Print ISSN"}) {
      push @issns, normalize_issn($result->{"Print ISSN"});
    }
    if ($result->{"Electronic ISSN"}) {
      push @issns, normalize_issn($result->{"Electronic ISSN"});
    }
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    $rec->{title} = $title;
    if ($usewikidatastart) {
      $rec->{start} ||= $start;
    } else {
      $rec->{start} = $start;
    }
    $rec->{end} = $end;
    $rec->{pkgid} = $result->{URL};
    # print "Looking at $title with $start and $end\n";
    push @recs, $rec;
  }
  return @recs;
}

sub museshortparser {
  my ($set, @results) = @_;
  return museparser($set, 0, @results);
}

sub muselongparser {
  my ($set, @results) = @_;
  return museparser($set, 1, @results);
}

sub simparser {
  my ($set, @results) = @_;
  my @recs = ();
  foreach my $result (@results) {
    my @issns = ();
    my $issn = $result->{ISSN};
    next if (!$issn || $issn eq "NULL" || !($issn =~ /\d/));
    push @issns, normalize_issn($issn);
    my $title = $result->{"Title"};
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    my $start = $result->{"First Volume"};
    my $end = $result->{"Last Volume"};
    $rec->{title} = $title;
    $rec->{start} = $start;
    $rec->{end} = $end;
    push @recs, $rec;
  }
  return @recs;
}

my $PQSEARCHSTUB = "https://search.proquest.com/publication/";

sub pqparser {
  my ($set, @results) = @_;
  foreach my $result (@results) {
    my $title = $result->{"Title"};
    my $pid = $result->{"Pub ID"};
    my $start = $result->{"Full Text (combined) First"};
    my $end = $result->{"Full Text (combined) Last"};
    my @issns = ();
    if ($result->{"ISSN"}) {
      push @issns, normalize_issn($result->{"ISSN"});
    }
    if ($result->{"eISSN"}) {
      push @issns, normalize_issn($result->{"eISSN"});
    }
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    $rec->{title} = $title;
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    if ($pid) {
      $rec->{pkgid} = $PQSEARCHSTUB . $pid;
    }
    if ($start =~ /(\d\d\d\d)/) {
      $rec->{start} = $1;
    }
    if ($end =~ /(\d\d\d\d)/) {
      $rec->{end} = $1;
    }
    push @recs, $rec;
  }
  return @recs;
}

sub wikipediaparser {
  my ($set, @results) = @_;
  my @recs = ();
  foreach my $result (@results) {
    my $start = $result->{startdate};
    # my $end = $result->{enddate} || $result->{otherenddate};
    my $end = $result->{enddate};
    my $title = $result->{idLabel};
    my $olbpid = $result->{olbpid};
    my $wdid = $result->{id};
    $wdid =~ s/.*\///;         # Wikidata URL is in file. Remove all before /
    my @issns = ();
    if ($result->{issns}) {
       @issns = split / /, $result->{issns};
    }
    my $rec = find_pub_record($olbpid, @issns);
    # print "Looking for $olbpid and found $rec\n";
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
    }
    $rec->{title} = $title;
    if (!$rec->{seenpubstart} ||
        !$rec->{start} || $rec->{start} > int($start)) { 
      $rec->{start} = int($start) || "";
      $rec->{seenpubstart} = 1;
    }
    $rec->{end} = ($end ? int($end) : "");
    $rec->{wikidata} = $wdid;
    $rec->{pkgid} = $result->{article};
    $rec->{description} = $result->{idDescription};
    push @recs, $rec;
  }
  return @recs;
}

sub pennparser {
  my ($set, @results) = @_;
  my @recs = ();
  foreach my $result (@results) {
    my $issnfield = $result->{ISSN};
    next if (!$issnfield);
    my ($start, $end);
    my $daterange = $result->{"Publication Date"};
    if ($daterange =~ /(\d\d\d\d).*-.*(\d\d\d\d)/) {
      ($start, $end) = ($1, $2);
    } elsif ($daterange =~ /-.*(\d\d\d\d)/) {
      $end = $1;
    } elsif ($daterange =~ /(\d\d\d\d).*-/) {
      $start = $1;
    } elsif ($daterange =~ /(\d\d\d\d)/) {
      $start = $1;
      # would be nice to be close out range, but source data not reliable enough
      #if ($result->{"Resource Type"} =~ /Ceased/) {
      #  $end = $start;
      #}
    }
    my $title = $result->{Title};
    my @rawissns = split /\s*;\s*/, $issnfield;
    #print "Keys are " . join (";", keys %{$result}) ."\n";
    #print "I found $title with $issnfield\n";
    my @issns = ();
    foreach my $issn (@rawissns) {
      if (!($issn =~ /^\d\d\d\d-?\d\d\d[\dXx]/)) {
        # silently skip digitless placeholders
        if ($issn =~ /\d/) {
          print STDERR "Unexpected ISSN for $title: $issn\n";
        }
        next;
      } else {
        push @issns, normalize_issn($issn);
      }
    }
    my $rec = find_pub_record(undef, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
      if (!$issnserial->{$issn}) {
        $issnserial->{$issn} = $rec;
      }
    }
    $rec->{title} = $title;
    $rec->{titlesort} = $result->{"Title (Normalized)"} || $title;
    $rec->{titlefull} = $result->{"Title (Complete)"} || $title;
    # print "starting with $start for $title\n";
    # if we've seen a journal before (in the publication file,
    #  not in the Wikidata file), only update start if it's earlier
    # 
    if (!$rec->{seenpubstart} ||
        !$rec->{start} || $rec->{start} > int($start)) { 
      $rec->{start} = int($start) || "";
      $rec->{seenpubstart} = 1;
    }
    $rec->{end} = ($end ? int($end) : "");
    $rec->{pkgid} = $franklin_issn_search . $issns[0];
    if (!$rec->{intable}) {
      push @recs, $rec;
      $rec->{intable} = 1;
    }
    if ($rec->{issns}) {
      $rec->{lowissn} = (sort keys %{$rec->{issns}})[0];
      # print STDERR "$rec->{title}has $rec->{lowissn}\n";
    }
    # print "$rec: start for $rec->{title} () is $rec->{start}, end is $rec->{end}\n";
  }
  return @recs;
}

my $hathirecordprefix = "https://catalog.hathitrust.org/Record/";

sub htfilesparser {
  my ($set, @results) = @_;
  my @recs = ();
  my $hathisummary;
  if ($set eq "hathitrust-us") {
    $hathisummary = readhathisummary($hathiussummaryfile);
  } else {
    $hathisummary = readhathisummary($hathisummaryfile);
  }
  foreach my $result (@results) {
    my $wdid = $result->{wikidataid};
    $wdid =~ s/.*\///;         # Wikidata URL is in file. Remove all before /
    my $olbp = $result->{olbpid};
    my $start = $result->{start_date};
    my $end = $result->{end_date};
    my @issns = split /\s+/, $result->{issns};
    my @hathiids = split /\s+/, $result->{hathiids};
    my @oclcids = split /\s+/, $result->{oclcs};
    my $rec = find_pub_record($olbp, @issns);
    if (!$rec) {
      $rec = {};
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
      $rec->{hathiline} ||= $hathisummary->{issnkey}->{$issn};
    }
    if (!$rec->{hathiline} && @hathiids) {
      foreach my $hid (@hathiids) {
        $rec->{hathiline} ||= $hathisummary->{hkey}->{$hid};
      }
    }
    if (!$rec->{hathiline} && @oclcids) {
      foreach my $oid (@oclcids) {
        # print "Assigning a hathiline for OCLC ID $oid\n";
        $rec->{hathiline} ||= $hathisummary->{okey}->{$oid};
      }
    }
    if ($rec->{hathiline}) {
      $rec->{title} ||= $rec->{hathiline}->{title};
      $rec->{pkgid} ||= $hathirecordprefix . $rec->{hathiline}->{prefid};
      $start = $rec->{hathiline}->{min};
      $end = $rec->{hathiline}->{max};
    }
    $rec->{wikidata} ||= $wdid;
    $rec->{start} ||= $start;
    $rec->{start} = ($rec->{start} ? int($rec->{start}) : "");
    $rec->{end} ||= $end;
    $rec->{end} = ($rec->{end} ? int($rec->{end}) : "");
    push @recs, $rec;
  }
  return @recs;
}

sub htpilotparser {
  my ($set, @results) = @_;
  my @recs = ();
  my ($hathiolbp, $hathiissns) = readhathipilotfile($hathipilot_wdquery);
  foreach my $result (@results) {
    my $title = $result->{title};
    my $htid = $result->{htid};
    $htid =~ s/^0*//;
    my $start = int($result->{start_date});
    my $end = int($result->{end_date});
    my $rec = find_pub_record($hathiolbp->{$htid},
                              keys %{$hathiissns->{$htid}});
    if (!$rec) {
      $rec = {};
    # } else {
    #  print "Found a rec for $htid!\n";
    }
    $rec->{title} = $title;
    $rec->{start} = int($rec->{start});
    if ($rec->{end}) {
      $rec->{end} = int($rec->{end});
    }
    if ($start) {
      if (!$rec->{start} || $rec->{start} > $start) {
        $rec->{start} = $start;
      }
    }
    if ($end && ($end != 9999)) {
      if (!$rec->{end} || $rec->{end} < $end) {
        $rec->{end} = $end;
      }
    }
    $rec->{pkgid} ||= $hathirecordprefix . $htid;
    push @recs, $rec;
  }
  return @recs;
} 

# pre64andolbp is a filter that returns records in the provided set
# that have coverage before the end of copyright renewals (i.e. pre-1964)
# or that have coverage listed in The Online Books Page

sub pre64andolbp {
  my (@recs) = @_;
  my @filteredrecs = ();
  foreach my $rec (@recs) {
    next if (!$rec->{issns});
    if ($rec->{end} && $rec->{end} <= $renewals_end) {
      push @filteredrecs, $rec;
    } elsif ($rec->{start} && $rec->{start} <= $renewals_end) {
      push @filteredrecs, $rec;
    } elsif ($rec->{coverage}) {
      push @filteredrecs, $rec;
    }
  }
  return @filteredrecs;
}

# Oxford no longer puts titles in alpha order; also has CSV-like title quotes 
sub oxfordfilter {
  my (@recs) = @_;
  foreach my $rec (@recs) {
    $rec->{title} =~ s/^\s*\"(.*)\"\s*$/$1/;
  }
  @recs = sort_titles(@recs);
  return pre64andolbp(@recs);
}

sub sortedfilter64 {
  my (@recs) = @_;
  @recs = pre64andolbp(@recs);
  return sort_titles(@recs);
}

sub sortedfilterwitholbp {
  my (@recs) = @_;
  my @filteredrecs = ();
  foreach my $rec (@recs) {
    next if (!$rec->{olbp});  # Needs OLBP record, but not necessarily ISSN
    if ($rec->{end} && $rec->{end} <= $renewals_end) {
      push @filteredrecs, $rec;
    } elsif ($rec->{start} && $rec->{start} <= $renewals_end) {
      push @filteredrecs, $rec;
    } elsif ($rec->{coverage}) {
      push @filteredrecs, $rec;
    }
  }
  return sort_titles(@filteredrecs);
}

sub hathisortedfilter {
  my (@recs) = @_;
  my @filteredrecs = ();
  foreach my $rec (@recs) {
    next if (!$rec->{title});  # Anything in hathifile should have a title
    next if (!$rec->{pkgid});  # Anything in hathifile should have a title
    if ($rec->{olbp} || ($rec->{start} && ($rec->{start} <= $renewals_end))) {
      push @filteredrecs, $rec;
    } elsif ($rec->{hathiline} && $rec->{hathiline}->{open}) {
      push @filteredrecs, $rec;
    }
  }
  return sort_titles(@filteredrecs);
}


sub pennfilter {
  my (@recs) = @_;
  foreach my $rec (@recs) {
    $rec->{title} =~ s/\[PAYMENT RECORD\]//;
    $rec->{title} =~ s/\s+/ /g;
    $rec->{title} =~ s/^\s+//;
    $rec->{title} =~ s/\s+$//;
  }
  @recs = sort {$a->{titlesort} cmp $b->{titlesort}} @recs;
  my $size = scalar(@recs);
  for (my $i = 0; $i < $size; $i++) {
    if ($recs[$i]->{titlefull}) {
      if (($i > 0 && 
           ($recs[$i]->{lowissn} ne $recs[$i-1]->{lowissn}) &&
           ($recs[$i]->{titlesort} eq $recs[$i-1]->{titlesort}))
         || 
          (($i < $size-1) &&
           ($recs[$i]->{lowissn} ne $recs[$i+1]->{lowissn}) &&
           ($recs[$i]->{titlesort} eq $recs[$i+1]->{titlesort}))) {
        # print STDERR " changing $recs[$i]->{title} to $recs[$i]->{titlefull}\n";
        $recs[$i]->{title} = $recs[$i]->{titlefull};
      }
    }
  }
  # recs = sort_titles(@recs);
  return pre64andolbp(@recs);
}


# everything is a filter that returns all records 

sub everything {
  my (@recs) = @_;
  return @recs;
}
  

sub addissns {
  my ($str, $rec) = @_;
  while ($str =~ /(\d\d\d\d\-\d\d\d[\dXx])(.*)/) {
    my $issn = uc($1);
    $issnserial->{$issn} = $rec;
    $rec->{issns}->{$issn} = 1;
    $str = $2;
  }
}

sub addcoverage {
  my ($str, $rec) = @_;
  push @{$rec->{coverage}}, $str;
  if ($str eq "present") {
    # special case if SREF is just present: include the current year
    addcoverage($OLBP::currentyear, $rec);
  }
  if ($str =~ /hathitrust.org/) {
    $rec->{hashathilink} = 1;
  }
}

sub coverage_summary {
  my ($rec) = @_;
  my @range = ();
  my $openend = 0;
  my $min = 10000;
  my $max = 0;
  # print "Coverage for $rec->{olbp} is $rec->{coverage}\n";
  return undef if (!$rec->{coverage});
  foreach my $part (@{$rec->{coverage}}) {
    my @bits = split /,/, $part;
    foreach my $bit (@bits) {
      next if ($bit eq "partial");
      if ($bit =~ /(\d+)-(\S+)/) {
        my ($start, $end) = ($1, $2);
        # print "I found $start and $end for $rec->{olbp}\n";
        if ($end =~ /present|recent|previous|current/) {
          $openend = $end;
          $range[$start] = 1;
          if (!$max || $max < $start) {
            $max = $start;
          }
        } elsif ($start < $end) {
          for (my $i = $start; $i <= $end; $i++) {
            $range[$i] = 1;
          }
        }
        if ($start < $min) {
          $min = $start;
        }
        if (int($end) && $end > $max) {
          $max = $end;
        }
      } elsif ($bit =~ /present|recent|previous|current/) {
        $openend = $bit;
      } else {
        $range[$bit] = 1;
        if ($bit < $min) {
          $min = $bit;
        }
        if ($bit > $max) {
          $max = $bit;
        }
      }
    }
  }
  if ($min && $max && ($min <= $max)) {
    my $gaps = 0;
    my $str = "";
    for (my $i = $min; $i <= $max; $i++) {
      if (!$range[$i]) {
        $gaps = 1;
        last;
      }
    }
    if ($openend) {
      $str = "$min-$openend";
    } elsif ($min < $max) {
      $str = "$min-$max";
    } else {
      $str = "$min";
    }
    if ($gaps) {
      $str =~ s/\-/$gapcode/;
    }
    if ($rec->{plus}) {
      $str .= "+";
    }
    return $str;
  }
  return undef;
}

sub reportrecord {
  my ($rec) = shift;
  return undef if (!$rec);
  print "$rec->{olbp}";
  if ($rec->{issns}) {
    print " (" .  join (", ", sort keys %{$rec->{issns}}) . ")";
  }
  print ": $rec->{title}";
  my $summary = coverage_summary($rec);
  if ($summary) {
    print " ($summary)";
  }
  print "\n";
}

sub readbookfile {
  my ($bookfile, $verbose) = @_;
  open IN, "< $bookfile" or die "Can't read $bookfile";
  binmode IN, ":utf8";
  my $id = "";
  my $rec;
  while (<IN>) {
    # Note that this depends on REF line appearing first in the record
    if (/^REF.*onlinebooks.*upenn.*id=(\S+) .*serial archives\s+$/) {
      $id = $1;
      $rec = {};
      $olbpserial->{$id} = $rec;
      $rec->{olbp} = $id;
    } elsif ($id && /^TITLE\s+(.*\S)\s*$/) {
      my $title = $1;
      $title = OLBP::Entities::utf8ify_entities($title);
      $rec->{title} = $title;
    } elsif ($id && /^ISSN\s+(.*\S)\s*$/) {
      addissns($1, $rec);
    } elsif ($id && /^SREF\s+(\S+)/) {
      addcoverage($1, $rec);
    } elsif ($id && /^SREL\s+(\S+)/) {
      $rec->{plus} = 1;
    } elsif (/^\S*$/) {
      if ($id && $verbose) {
        reportrecord($rec);
      }
      $id = "";
      $rec = {};
    }
  } 
  if ($id && $verbose) {
     reportrecord($rec);
  }
  close IN;
}

sub readbadissns {
  my ($filename) = @_;
  open my $fh, "<", $filename or die "filename: $!";
  while (<$fh>) {
    s/#.*//;
    if (/(\d\d\d\d-\d\d\d[\dxX])\s*\-\>\s*(\d\d\d\d-\d\d\d[\dxX])/) {
      my ($badissn, $goodissn) = ($1, $2);
      $badissnmap->{$badissn} = $goodissn;
    } elsif (/^(\d\d\d\d-\d\d\d[\dxX])\s*\-\>\s*([a-z0-9]+)/) {
      my ($badissn, $olbpid) = ($1, $2);
      $badissnid->{$badissn} = $olbpid;
    }
  }
  close $fh;
}

sub readcsvfile {
  my ($filename, $skiplines) = @_;
  my @results;
  #open my $fh, "<:encoding(utf8)", $filename or die "filename: $!";
  open my $fh, "<", $filename or die "filename: $!";
  # print "Filename is $filename\n";
  while (int($skiplines)) {
    my $row = $csv->getline($fh);
    $skiplines -= 1;
  }
  my $row = $csv->getline($fh);
  my @fieldnames = @{$row};
  if ($fieldnames[0] =~ /\x{feff}(.*)/) {
      # remove byte order marker if present
      $fieldnames[0] = $1;
  }
  # print "Fieldnames are " . join ("; ", @fieldnames) . "\n" if ($filename =~ /penn/);
  while (my $row = $csv->getline ($fh)) {
    my %hash = ();
    my @fields = @{$row};
    for (my $i = 0; $i < scalar(@fields); $i++) {
      $hash{$fieldnames[$i]} = $fields[$i];
      # print "Putting $fields[$i] in $fieldnames[$i]\n" if ($filename =~ /penn/);
    }
    push @results, \%hash;
  }
  $csv->eof or $csv->error_diag();
  close $fh;
  return @results;
}

# The Wikipedia file is a CSV file, but we need to unmess
# the non-ASCII encoding.

sub readwikipediafile {
  my ($filename) = @_;
  my @results = readcsvfile($filename);
  foreach my $result (@results) {
    if ($result->{idLabel} =~ /[^\x00-\x7f]/) { 
      utf8::decode($result->{idLabel});
    }
  }
  return @results;
}

# to sort titles, take away leading spaces, diacritics,
# certain initial punctuation, and certain initial articles

sub sort_titles {
  my (@recs) = @_;
  foreach my $rec (@recs) {
    my $startcode = ord(substr($rec->{title}, 0, 1));
    if ($startcode > 0x3000 || ($startcode >= 0x590 && $startcode < 0x600)) {
      # Don't attempt to normalize CJK or Hebrew titles for sorting
      $rec->{titlesort} = $rec->{title};
      # print STDERR "Not trying to mess with $rec>{title}\n";
      next;
    }
    my $key = lc(OLBP::Entities::normalize_utf8($rec->{title}));
    $key =~ s/^["'!\?\]\[]//;
    $key =~ s/^\s*//;
    $key =~ s/^(a|der|die|il|la|le|les|t|the|y)\s+//;
    $key =~ s/^l' ?//;
    $key =~ s/^ha-//;
    $key =~ s/^al[ \-]'?//;
    $key =~ s/^el (?!(paso|dorado))//;  # "El" stays for El Paso and El Dorado
    $key =~ s/^los (?!angeles)//;       # "Los" stays for Los Angeles
    $key =~ s/^\(//;                    # ignore parens after articles
    $key =~ s/^\s*\.\.\.\s*//;          # and ellipses after articles
    # print "key for $rec->{title} is '$key'\n";
    $rec->{titlesort} = $key;
  }
  return sort {$a->{titlesort} cmp $b->{titlesort}} @recs;
}

# The Wikipedia filter doesn't actually eliminate items
# (they're prefiltered) but it does sort them and add descriptions
# when necessary to disambiguate between adjacent items

sub wikipedia_filter {
  my (@recs) = sort_titles(@_);
  my $size = scalar(@recs);
  for (my $i = 0; $i < $size; $i++) {
    if ($recs[$i]->{description}) {
      if (($i > 0 && 
           ($recs[$i]->{wikidata} ne $recs[$i-1]->{wikidata}) &&
           ($recs[$i]->{titlesort} eq $recs[$i-1]->{titlesort}))
         || 
          (($i < $size-1) &&
           ($recs[$i]->{wikidata} ne $recs[$i+1]->{wikidata}) &&
           ($recs[$i]->{titlesort} eq $recs[$i+1]->{titlesort}))) {
        $recs[$i]->{title} .= " ($recs[$i]->{description})";
      }
    }
  }
  return @recs;
}

# MUSE files are TSV files, but we need to skip the first 3 lines

sub readmusefile {
  my ($filename, %params) = @_;
  $params{skiplines} = 3;
  return readtsvfile($filename, %params);
}

sub readtsvfile {
  my ($filename, %params) = @_;
  my @results;
  my $encoding = $params{encoding} || "utf8";
  open my $fh, "<:encoding($encoding)", $filename or die "$filename: $!";
  if ($params{skiplines}) {
    for (my $i = 0; $i < $params{skiplines}; $i++) {
      my $discard = <$fh>;
    }
  }
  my $row = <$fh>;
  my @fieldnames = split /\t/, $row;
  if ($fieldnames[0] =~ /\x{feff}(.*)/) {
      # remove byte order marker if present
      $fieldnames[0] = $1;
  }
  while (my $row = <$fh>) {
    my %hash = ();
    my @fields = split /\t/, $row;
    for (my $i = 0; $i < scalar(@fields); $i++) {
      my $val = $fields[$i];
      $val =~ s/\n/ /g;              # shouldn't trigger unless RS has changed
      $hash{$fieldnames[$i]} = $val;
    }
    push @results, \%hash;
  }
  close $fh;
  return @results;
}

# An Oxford file is like a TSV file, except that lines ends with CRs

sub readoxfordfile {
  my ($filename, %params) = @_;
  my $oldslash = $/;                            # remember old file separator
  $/ = "\r";                                    # and reset it to CR
  my @results = readtsvfile($filename, %params);
  $/ = $oldslash;                               # now put it back
  return @results;
}

# Proquest files have 3 extra lines at start, and 2 extra at end

sub readpqfile {
  my ($filename, %params) = @_;
  $params{skiplines} = 3;
  $params{encoding} = "cp-1252";
  my @results = readtsvfile($filename, %params);
  pop @results;
  pop @results;
  return @results;
}


# Some routines to clean up UTF-8 characters that got mangled in translation

sub unmess_utf8 {
  my $str = shift;
  $str =~ s/\x81/\xfc/g;    # u with umlaut
  $str =~ s/\x82/\xe9/g;    # e with acute accent
  $str =~ s/\x84/\xe4/g;    # a with umlaut
  $str =~ s/\xE1/\xdf/g;    # sharp s (the German letter resembling B)
  return $str;
}

# A Wiley file is like a TSV file, but its
#  field names need to have spaces stripped
#  and the character encoding needs to be fixed
#  and some journal titles unquoted

sub readwileyfile {
  my ($filename, %params) = @_;
  my @results;
  open my $fh, "<:encoding(utf8)", $filename or die "filename: $!";
  # uncomment above when troubleshooting character encodings
  # and put in extra backslash in fixes below
  # open my $fh, "<", $filename or die "filename: $!";
  my $row = <$fh>;
  $row =~ s/ //g;                     # eliminate spaces in field names
  my @fieldnames = split /\t/, $row;
  while (my $row = <$fh>) {
    #  have to fix mis-encoded non-ASCII characters
    $row = unmess_utf8($row);
    my %hash = ();
    my @fields = split /\t/, $row;
    for (my $i = 0; $i < scalar(@fields); $i++) {
      if ($fieldnames[$i] eq "publication_title") {
        $fields[$i] =~ s/^\"(.*)\"$/$1/;
      }
      #  dequote completely 
      $hash{$fieldnames[$i]} = $fields[$i];
    }
    push @results, \%hash;
  }
  close $fh;
  return @results;
}

# A Springer file is like a TSV file, but may need to be opened
# with correct encoding, and have quoted title names dequoted

sub readspringerfile {
  my ($filename, %params) = @_;
  my @results;
  # open my $fh, "<:encoding(utf-16)", $filename or die "filename: $!";
  open my $fh, "<:encoding(utf-8)", $filename or die "filename: $!";
  # open my $fh, "<", $filename or die "filename: $!";
  my $row = <$fh>;
  my @fieldnames = split /\t/, $row;
  if ($fieldnames[0] =~ /\x{feff}(.*)/) {
      # remove byte order marker if present
      $fieldnames[0] = $1;
  }
  while (my $row = <$fh>) {
    my %hash = ();
    my @fields = split /\t/, $row;
    for (my $i = 0; $i < scalar(@fields); $i++) {
      if ($fieldnames[$i] eq "publication_title") {
        $fields[$i] =~ s/^\"(.*)\"$/$1/;
      }
      $hash{$fieldnames[$i]} = $fields[$i];
    }
    push @results, \%hash;
  }
  close $fh;
  return @results;
}

# Sage files are available as Excel files.  I load them into Excel
# on a Mac and then save them as TSV text files, which renders them
# with carriage-returns as line separators

sub readsagefile {
  my ($filename, %params) = @_;
  my @results;
  open my $fh, "<", $filename or die "filename: $!";
  #my $content = <$fh>;
  #my @rows = split /\r/, $content;
  my @rows = <$fh>;
  #print STDERR "There are " . scalar(@rows) . "rows\n";
  my $fieldrow = shift @rows;
  $fieldrow =~ s/[\r\n]//g;
  my @fieldnames = split /\t/, $fieldrow;
  if ($fieldnames[0] =~ /\x{feff}(.*)/) {
      # remove byte order marker if present
      $fieldnames[0] = $1;
  }
  #$sagetitlefield = $fieldnames[0];   # Title field comes first; name varies
  $sagetitlefield = $fieldnames[1];   # Title field comes second; name varies
  while (my $row = shift @rows) {
    #  have to fix mis-encoded non-ASCII characters
    $row =~ s/\x9f/\xfc/g;    # u with umlaut
    $row =~ s/\xd5/'/g;       # apostrophe
    my %hash = ();
    my @fields = split /\t/, $row;
    last if (!$fields[0]);                       # blank line ends real data
    for (my $i = 0; $i < scalar(@fields); $i++) {
      if ($fieldnames[$i] eq $sagetitlefield) { 
        $fields[$i] =~ s/^\"(.*)\"$/$1/;
      }
      $hash{$fieldnames[$i]} = $fields[$i];
      # print STDERR "$fieldnames[$i] gets $fields[$i]\n";
    }
    push @results, \%hash;
  }
  close $fh;
  return @results;
}

# SIM file was available as an Excel file.  I loaded it into Excel
# on a Mac and then save them as TSV text files, which renders them
# with carriage-returns as line separators.  There's an extra CR in the
# first row, but it's to fields I don't need

sub readsimfile {
  my ($filename, %params) = @_;
  my @results;
  open my $fh, "<", $filename or die "filename: $!";
  my $content = <$fh>;
  my @rows = split /\r/, $content;
  my @fieldnames = split /\t/, (shift @rows);
  if ($fieldnames[0] =~ /\x{feff}(.*)/) {
      # remove byte order marker if present
      $fieldnames[0] = $1;
  }
  while (!($rows[0] =~ /^\d/)) {
    # first real row should start with a digit
    shift @rows;
  }
  while (my $row = shift @rows) {
    #  may have to fix mis-encoded non-ASCII characters
    # $row =~ s/\xd5/'/g;       # apostrophe
    my %hash = ();
    my @fields = split /\t/, $row;
    last if (!$fields[0]);                       # blank line ends real data
    for (my $i = 0; $i < scalar(@fields); $i++) {
      $fields[$i] =~ s/^\"(.*)\"$/$1/;
      $hash{$fieldnames[$i]} = $fields[$i];
    }
    push @results, \%hash;
  }
  close $fh;
  return @results;
}

sub find_pub_record_old {
  my ($olbpid, @issns) = @_;
  # print "Looking for $olbpid and " . join (", ", @issns). "\n";
  if ($olbpid && $olbpserial->{$olbpid}) {
    return $olbpserial->{$olbpid};
  }
  foreach my $issn (@issns) {
    next if (!$issn);
    if ($issnserial->{$issn}) {
      return $issnserial->{$issn};
    }
  }
  foreach my $issn (@issns) {
    next if (!$issn);
    my $betterissn = $badissnmap->{$issn};
    if ($betterissn && $issnserial->{$betterissn}) {
      # print STDERR "found $betterissn for $issn\n";
      return $issnserial->{$betterissn};
    }
  }
  return undef;
}

sub find_pub_record {
  my ($olbpid, @issns) = @_;
  # print "Looking for $olbpid and " . join (", ", @issns). "\n";
  if ($olbpid && $olbpserial->{$olbpid}) {
    return $olbpserial->{$olbpid};
  }
  foreach my $issn (@issns) {
    next if (!$issn);
    my $betterissn = $badissnmap->{$issn};
    if ($betterissn && $issnserial->{$betterissn}) {
      # print STDERR "found $betterissn for $issn\n";
      return $issnserial->{$betterissn};
    }
  }
  foreach my $issn (@issns) {
    next if (!$issn);
    if ($issnserial->{$issn}) {
      return $issnserial->{$issn};
    }
  }
  return undef;
}

# Dates in Wikidata get overspecific; remove all that could be introduced

sub simplifywikidate {
  my ($str) = @_;
  $str =~ s/-01-01T00:00:00Z//;
  $str =~ s/-01T00:00:00Z//;
  $str =~ s/T00:00:00Z//;
  return $str;
}

sub readwikidata {
  my ($wikidatafile) = @_;
  my @publications = readcsvfile($wikidatafile);
  # print "I know of " . scalar(@publications) . " publications!\n";
  foreach my $pub (@publications) {
    my $wduri  = $pub->{wikidataid};
    my $olbpid = $pub->{olbpid};
    # print "OLBPID is $olbpid\n" if ($olbpid);
    my $wpuri  = $pub->{article};
    my $start  = ($pub->{start} ? int($pub{start}) : $pub->{start});
    my $end  = ($pub->{end} ? int($pub{end}) : $pub->{end});
    my @issns  = split /\s+/, $pub->{issns};
    # skip ones that will have better substitutes (not sure this works)
    # next if (scalar(@issns) == 1 && $badissnmap->{$issns[0]});
    my $wdid = $wduri;
    $wdid =~ s/.*wikidata.org.entity.//;
    my $wptitle = $wpuri;
    $wptitle =~ s/.*wikipedia.org.wiki.//;
    my $rec = find_pub_record($olbpid, @issns);
    if (!$rec) {
      $rec = {};
      $rec->{olbp} = $olbpid;
    }
    foreach my $issn (@issns) {
      $rec->{issns}->{$issn} = 1;
      if (!$issnserial->{$issn}) {
        $issnserial->{$issn} = $rec;
      }
    }
    if ($olbpid && (!$olbpserial->{$olbpid})) {
      # print "Adding $olbpid to $rec\n";
      $olbpserial->{$olbpid} = $rec;
    }
    $rec->{wikidata} ||= $wdid;
    $rec->{wikipedia} ||= $wptitle;
    $rec->{start} ||= simplifywikidate($pub->{start});
    $rec->{end} ||= simplifywikidate($pub->{end});
  }
}

sub readhathisummary {
  my ($hathifile) = @_;
  my $hathisummary;
  open HATHISUM, "< $hathifile" or die "Can't open Hathi summary $hathifile\n";
  binmode HATHISUM, ":utf8";
  while (my $line = <HATHISUM>) {
    chop $line;
    my $linerec = {};
    my @fields = split /\|/, $line;
    my $issn = $fields[0];
    my $title = $fields[6];
    $title =~ s/[\x80-\xa1\xa6\xa8\xac\x{202a}]//g;  # filter some bad chars
    if ($issn) {
      $linerec->{issn} = $issn;
    }
    $linerec->{open} = $fields[3];
    $linerec->{min} = $fields[4];
    $linerec->{max} = $fields[5];
    $linerec->{title} = $title;
    my @hathiids = split /\s+/, $fields[1];
    my @oclcids = split /\s+/, $fields[2];
    $linerec->{prefid} = $hathiids[0];
    if ($issn) {
      $hathisummary->{issnkey}->{$issn} = $linerec;
    }
    foreach my $hid (@hathiids) {
      $hathisummary->{hkey}->{$hid} = $linerec;
    }
    foreach my $oid (@oclcids) {
      # print "Adding $oid (for iSSN $issn)\n";
      $hathisummary->{okey}->{$oid} = $linerec;
    }
  }
  close HATHISUM;
  return $hathisummary;
}

sub readhathipilotfile {
  my ($hathifile) = @_;
  my @rows = readcsvfile($hathifile);
  my $hathiolbp;
  my $hathiissns;
  foreach my $row (@rows) {
    my $olbpid = $row->{olbpid};
    my $hathiids = $row->{hathitrustids};
    my $issn = $row->{issn};
    my $wikidataid = $row->{wikidataid};
    my $qid = "";
    if ($wikidataid =~ /(Q.*)/) {
      $qid = $1;
    }
    foreach my $hathiid (split /\s+/, $hathiids) {
      $hathiid =~ s/^0*//;
      if ($olbpid) {
        $hathiolbp->{$hathiid} = $olbpid;
      }
      if ($issn) {
        $hathiissns->{$hathiid}->{$issn} = 1;
      }
    }
  }
  return ($hathiolbp, $hathiissns);
}

sub readserialset {
  my ($serialset) = @_;
  my $setrec = $tables->{$serialset};
  my @results = ();
  if (!$setrec) {
    print STDERR "Unknown set: $serialset\n";
    exit 0;   # no point in going further
    # return undef;
  }
  my $filename = $setrec->{file};
  my $reader = $setrec->{reader};
  my $parser = $setrec->{parser};
  if ($filename) {
    @results = $reader->($filename);
  } elsif ($setrec->{filepattern}) {
    my @names = glob($setrec->{filepattern});
    print join '\n', @names;
    exit 0;
  }
  my @recs = $parser->($serialset, @results);
  return @recs;
}

sub filterserialset {
  my ($serialset, @recs) = @_;
  my $setrec = $tables->{$serialset};
  return @recs if (!$serialset);
  my $filter = $setrec->{filter};
  return @recs if (!$filter);
  @recs = $filter->(@recs);
  return @recs;
}

sub link_recs_with_no_valid_issns {
  foreach my $badissn (keys %{$badissnid}) {
    if ($issnserial->{$badissn}) {
      my $rec = $issnserial->{$badissn};
      if (!$rec->{olbp}) {
        $rec->{olbp} = $badissnid->{$badissn};
        # print STDERR "Assigning OLBP record $badissnid->{$badissn} to record with iSSN $badissn\n";
      }
    }
  }
}

sub tsv_line {
  my ($arrayref) = @_;
  my $str = "";
  my @list = ();
  foreach my $item (@{$arrayref}) {
    $item =~ s/\t/ /g;
    push @list, $item;
  }
  $str = join("\t", @list);
  $str .= "\n";
}

# ISSN (lowest) | Title | Dates | Wiki (WD+WP) | Copyright | Free Online

sub maketable {
  my (@recs) = @_;
  my %seen = ();
  # binmode STDOUT, ":utf8";
  my $cinfo = new OLBP::CopyrightInfo(dir=>$cinfodir);
  my $arrayref = ["ISSN", "Internal ID", "Online Books ID", "Title", "Dates", "Wikidata ID", "First renewal year", "Free issue years"];
  my $str = tsv_line($arrayref);
  # $csv->print(STDOUT, $arrayref);
  # print "There are " . scalar(@recs) . " records\n";
  foreach my $rec (@recs) {
    # print "Seen it ($rec)\n" if ($seen{$rec});
    next if ($seen{$rec});  # we sometimes have redundant source file journals
    $seen{$rec} = 1;
    my $lowissn = "";
    if ($rec->{issns}) {
      $lowissn = (sort keys %{$rec->{issns}})[0];
    }
    my $title = $rec->{title};
    # print "Entering with $rec and title $title\n";
    next if (!$title);
    my $dates = "";
    if ($rec->{start} || $rec->{end}) {
      $dates = $rec->{start} . "-" . $rec->{end};
    }
    my $wiki = $rec->{wikidata};
    my $pkgid = $rec->{pkgid};
    my $olbpid = $rec->{olbp};
    my $freerange = coverage_summary($rec);
    my $copyright = "Unknown";
    if ($olbpid) {
      my $rec = $cinfo->get_json(filename=>$olbpid);
      my $result = $cinfo->firstrenewal_year(json=>$rec);
      if ($result) {
        $copyright = $result;
        if (scalar($cinfo->additional_notes(json=>$rec))) {
          $copyright .= "*";
        } elsif (scalar($cinfo->preceding_ids(json=>$rec)) ||
                 scalar($cinfo->succeeding_ids(json=>$rec)) ||
                 scalar($cinfo->related_ids(json=>$rec))) {
           # for now, we just declare it complicated if there
           # are any preceding, succeeding, or related records
           # Later, we'll get smarter about following paths
           $copyright .= "*";
        }
      }
      if (!$freerange && $cinfo->online_predecessor(json=>$rec)) {
        $freerange = "Predecessor";
      }
    }
    if (!$freerange &&
         $rec->{hathiline} && $rec->{hathiline}->{open}) { 
      # Used to link HathiTrust serials that are open but not in OLBP yet
      $freerange = "Available";
    }
    if ($copyright eq "Unknown") { 
      if (int($rec->{end}) && $rec->{end} < $renewals_start) {
        $copyright = "N/A";
      } elsif (int($rec->{start}) && $rec->{start} > $renewals_end) {
        $copyright = "All";
      }
    }
    my $bestissn = $lowissn;
    if ($badissnmap->{$bestissn}) {
      $bestissn = $badissnmap->{$bestissn};
    }
    # print "\"$lowissn\",";  # force quotes around ISSN so Excel behaves better

    $arrayref = [$bestissn,$pkgid,$olbpid,$title,
                 $dates,$wiki,$copyright,$freerange];
    $str .= tsv_line($arrayref);
    # $csv->print(STDOUT, $arrayref);
  }
  return $str;
}

sub tsvtocsv {
  my ($tsvstr, $dopending) = @_;
  my @csvlines = ();
  my @lines = split /\n/, $tsvstr;
  my $pending;
  if ($dopending) {
    $pending = new OLBP::PendingSerials(dir=>$pooldir);
  }
  foreach my $line (@lines) {
    my @itemlist = ();
    my @items = split /\t/, $line;
    if ($pending) {
       my $statuscol = 6;
       my $issn = $items[0];
       my $title = $items[3];
       my $status = $items[$statuscol];
       if ($status eq "Unknown" &&
           $pending->is_pending(issn=>$issn, title=>$title)) {
          $items[$statuscol] = "Pending";
       }
    }
    foreach my $item (@items) {
      $item =~ s/\n/ /g;
      $item =~ s/\r/ /g;
      if ($item =~ /"/) {
        $item =~ s/"/""/g;
        $item = "\"$item\"";
      } elsif ($item =~ /,/) {
        $item = "\"$item\"";
      }
      push @itemlist, $item;
    }
    push @csvlines, join (",", @itemlist);
  }
  return join ("\r\n", @csvlines);
}

sub writetofile {
  my ($name, $string) = @_;
  open my $fh, "> $name" or die "Can't open $name for writing";
  binmode $fh, ":utf8";
  print $fh $string;
  close $fh;
}

sub updateset {
  my ($serialset, $tsv) = @_;
  my $setrec = $tables->{$serialset};
  my $dir = $setdir . $serialset;
  my $tsvfile = $dir . "/$serialset.tsv";
  writetofile($tsvfile, $tsv);
  my $csv = tsvtocsv($tsv, 1);
  my $csvfile = $dir . "/$serialset.csv";
  writetofile($csvfile, $csv);
  my $statusfile = $dir . "/status";
  my $sourcedate = strftime "%B %e, %Y",
                   localtime((stat ($setrec->{file}))[9]);
  my $nowdate = strftime "%B %e, %Y", localtime();
  my $statusstr = "Table generated $nowdate " .
                  "using provider data downloaded $sourcedate.";
  writetofile($statusfile, "$statusstr\n");
}

binmode STDOUT, ":utf8";

my $tableonly = 0;

if ($ARGV[0] eq "table") {
  $tableonly = 1;
  shift @ARGV;
}

if (!scalar(@ARGV)) {
  die "Usage: serialtable [table] <serialset>";
}
my $serialset = $ARGV[0];

readbadissns($badissnfile);
readbookfile($bookfile);
readwikidata($wikidatafile);
my @recs = readserialset($serialset);
link_recs_with_no_valid_issns();
@recs = filterserialset($serialset, @recs);
my $str = maketable($tableonly, @recs);
if ($tableonly) {
  print $str;
} else {
  updateset($serialset, $str);
}
